import json
import urllib.request
import urllib.error
import csv

# GitHub personal access token
token = "your_github_personal_access_token"

# Headers for API requests (authentication with token)
headers = {
    "Authorization": f"token {token}",
    "Accept": "application/vnd.github.v3+json"
}

# Function to fetch data from GitHub API
def fetch_data(url):
    req = urllib.request.Request(url, headers=headers)
    try:
        with urllib.request.urlopen(req) as response:
            return response.read()
    except urllib.error.HTTPError as e:
        print(f"HTTP Error [{e.code}]: {e.reason}")
        return None

# Function to get organizations starting with "TPO-"
def get_tpo_organizations():
    orgs_url = "https://api.github.com/user/orgs"
    orgs_data = fetch_data(orgs_url)
    if not orgs_data:
        return []

    organizations = json.loads(orgs_data)
    return [org["login"] for org in organizations if org["login"].startswith("TPO-")]

# Function to process repositories and find workflow files
def process_repos(org_name):
    base_url = f"https://api.github.com/orgs/{org_name}/repos"
    repos_data = []
    page = 1

    while True:
        url = f"{base_url}?page={page}&per_page=100"  # Fetch 100 repos per page
        response = fetch_data(url)
        if not response:
            break
        page_repos = json.loads(response)
        if not page_repos:
            break
        repos_data.extend(page_repos)
        page += 1

    return repos_data

# Function to check workflow files for conditions and save results to CSVs
def check_workflow_files(org_name, repos_data):
    workflow_files_datapower = []
    workflow_files_other = []

    for repo in repos_data:
        repo_name = repo["name"]
        workflow_url = f"https://api.github.com/repos/{org_name}/{repo_name}/contents/.github/workflows"

        workflows_response = fetch_data(workflow_url)
        if not workflows_response:
            continue

        workflows_data = json.loads(workflows_response)
        for workflow_file in workflows_data:
            if workflow_file["type"] == "file" and workflow_file["name"].endswith(".yml"):
                file_download_url = workflow_file["download_url"]
                file_content = fetch_data(file_download_url).decode("utf-8")

                # Determine the category based on file name
                if workflow_file["name"] in ["datapower.yml", "datapowerpolicy.yml"]:
                    target_list = workflow_files_datapower
                else:
                    target_list = workflow_files_other

                is_checkmarx = "is_checkmarx: 'true'" if "is_checkmarx: 'true'" in file_content else \
                               "is_checkmarx: 'false'" if "is_checkmarx: 'false'" in file_content else \
                               "is_checkmarx: not present"

                org_scan = "orgScan: 'NFCU'" if "orgScan: 'NFCU'" in file_content else \
                           "orgScan: not present"

                target_list.append({
                    "orgname": org_name,
                    "reponame": repo_name,
                    "workflowname": workflow_file["name"],
                    "is_checkmarx": is_checkmarx,
                    "orgScan": org_scan
                })

    # Save results to CSVs
    with open("workflow_files_datapower.csv", "w", newline='') as csvfile1, \
         open("workflow_files_other.csv", "w", newline='') as csvfile2:
        fieldnames = ["orgname", "reponame", "workflowname", "is_checkmarx", "orgScan"]
        writer1 = csv.DictWriter(csvfile1, fieldnames=fieldnames)
        writer2 = csv.DictWriter(csvfile2, fieldnames=fieldnames)

        writer1.writeheader()
        writer2.writeheader()
        writer1.writerows(workflow_files_datapower)
        writer2.writerows(workflow_files_other)

# Main script execution
def main():
    tpo_organizations = get_tpo_organizations()
    for org_name in tpo_organizations:
        repos_data = process_repos(org_name)
        check_workflow_files(org_name, repos_data)

if __name__ == "__main__":
    main()
